{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 :Import Dataset\n",
    "In the code cell below, we import a dataset of actress images. We populate a few variables through the use of the load_files function from the scikit-learn library:\n",
    "\n",
    "1. train_files, valid_files, test_files - numpy arrays containing file paths to images\n",
    "2. train_targets, valid_targets, test_targets - numpy arrays containing onehot-encoded classification labels\n",
    "3. actress_names - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4970 total dog images.\n",
      "\n",
      "There are 3978 training actress images.\n",
      "There are 496 validation actress images.\n",
      "There are 496 test actress images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('../Celebs/train')\n",
    "valid_files, valid_targets = load_dataset('../Celebs/valid')\n",
    "test_files, test_targets = load_dataset('../Celebs/test')\n",
    "\n",
    "# load list of dog names\n",
    "# dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "#train_files -> paths to the files like dogImages/train/095.Kuvasz/Kuvasz_06442.jpg\n",
    "#train_targets -> 2d array of size*133 categorical all 0s one 1 based on which category the file belongs\n",
    "\n",
    "# print statistics about the dataset\n",
    "# print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training actress images.' % len(train_files))\n",
    "print('There are %d validation actress images.' % len(valid_files))\n",
    "print('There are %d test actress images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process the data\n",
    "Here we are using tensorflow as backend for keras and it requires our images as a certain 4D array a.k.a 4D Tensor with shape.\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where nb_samples corresponds to the total number of images (or samples), and rows, columns, and channels correspond to the number of rows, columns, and channels for each image, respectively.\n",
    "\n",
    "The path_to_tensor takes a string spacifying file location and it does the following operation.\n",
    "\n",
    "1. Resizes the image as (224,224).\n",
    "2. Convert the squared image as an array (3d array)\n",
    "3. Expand the 3d array to 4d array as (1,224,224,3)\n",
    "\n",
    "Another helper function paths_to_tensor takes an array of image file locations as param and in turn calls path_to_tensor on all of them and then vertically stack the output.\n",
    "\n",
    "Here, nb_samples is the number of samples, or number of images, in the supplied array of image paths. It is best to think of nb_samples as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3978/3978 [00:47<00:00, 83.42it/s]\n",
      "100%|██████████| 496/496 [00:05<00:00, 83.66it/s]\n",
      "100%|██████████| 496/496 [00:04<00:00, 100.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "1. Create a CNN from scratch with these tensors\n",
    "2. Create Augmentations and with use of that, And create bottleneck features and save it to S3\n",
    "3. Fetch bottleneck features from S3 and use transfer learning to build a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 2 Create CNN from the scratch\n",
    "Here we will create a CNN from the scratch using Keras. Note that we will not be using any models (vgg,resnet) we will take images as input and train our model.\n",
    "\n",
    "Our target here is to create simplistic model with training,test accuracy above 5%.\n",
    "\n",
    "Be careful with adding too many trainable layers! More parameters means longer training, which means you are more likely to need a GPU to accelerate the training process. Thankfully, Keras provides a handy estimate of the time that each epoch is likely to take; you can extrapolate this estimate to figure out how long it will take for your algorithm to train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 56, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 133)               8645      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 133)               0         \n",
      "=================================================================\n",
      "Total params: 19,189\n",
      "Trainable params: 19,189\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=(224,224,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(train_targets.shape[1], activation='softmax'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3978 samples, validate on 496 samples\n",
      "Epoch 1/50\n",
      "3978/3978 [==============================] - 17s 4ms/step - loss: 7.1750 - acc: 0.2293 - val_loss: 1.6070 - val_acc: 0.3085\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.60702, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 2/50\n",
      "3978/3978 [==============================] - 11s 3ms/step - loss: 7.3807 - acc: 0.2594 - val_loss: 1.5842 - val_acc: 0.3085\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.60702 to 1.58423, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 3/50\n",
      "3978/3978 [==============================] - 11s 3ms/step - loss: 7.1612 - acc: 0.2647 - val_loss: 1.5667 - val_acc: 0.3085\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.58423 to 1.56673, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 4/50\n",
      "3978/3978 [==============================] - 11s 3ms/step - loss: 6.9376 - acc: 0.2715 - val_loss: 1.5664 - val_acc: 0.3085\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.56673 to 1.56638, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 5/50\n",
      "3978/3978 [==============================] - 11s 3ms/step - loss: 7.2873 - acc: 0.2413 - val_loss: 1.5662 - val_acc: 0.3085\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.56638 to 1.56619, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 6/50\n",
      "3978/3978 [==============================] - 11s 3ms/step - loss: 7.0884 - acc: 0.2612 - val_loss: 1.5708 - val_acc: 0.3085\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.56619\n",
      "Epoch 7/50\n",
      "3978/3978 [==============================] - 11s 3ms/step - loss: 6.7606 - acc: 0.2728 - val_loss: 1.5668 - val_acc: 0.3085\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.56619\n",
      "Epoch 8/50\n",
      "3978/3978 [==============================] - 11s 3ms/step - loss: 7.1450 - acc: 0.2695 - val_loss: 1.5643 - val_acc: 0.3065\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.56619 to 1.56426, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 9/50\n",
      "3978/3978 [==============================] - 11s 3ms/step - loss: 7.2799 - acc: 0.2640 - val_loss: 1.5642 - val_acc: 0.3085\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.56426 to 1.56415, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 10/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.3188 - acc: 0.2511 - val_loss: 1.5734 - val_acc: 0.3004\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.56415\n",
      "Epoch 11/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1394 - acc: 0.2552 - val_loss: 1.5743 - val_acc: 0.3085\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.56415\n",
      "Epoch 12/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1616 - acc: 0.2662 - val_loss: 1.5580 - val_acc: 0.3085\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.56415 to 1.55803, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 13/50\n",
      "3978/3978 [==============================] - 11s 3ms/step - loss: 7.1725 - acc: 0.2702 - val_loss: 1.5877 - val_acc: 0.3085\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.55803\n",
      "Epoch 14/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0547 - acc: 0.2577 - val_loss: 1.5693 - val_acc: 0.3165\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.55803\n",
      "Epoch 15/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 6.9962 - acc: 0.2524 - val_loss: 1.5640 - val_acc: 0.3185\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.55803\n",
      "Epoch 16/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.3030 - acc: 0.2574 - val_loss: 1.5502 - val_acc: 0.3105\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.55803 to 1.55021, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 17/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1382 - acc: 0.2559 - val_loss: 1.5600 - val_acc: 0.3145\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.55021\n",
      "Epoch 18/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.2226 - acc: 0.2753 - val_loss: 1.5512 - val_acc: 0.3165\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.55021\n",
      "Epoch 19/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 6.9437 - acc: 0.2705 - val_loss: 1.5657 - val_acc: 0.3206\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.55021\n",
      "Epoch 20/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0826 - acc: 0.2687 - val_loss: 1.5623 - val_acc: 0.2984\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.55021\n",
      "Epoch 21/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0833 - acc: 0.2599 - val_loss: 1.5603 - val_acc: 0.3125\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.55021\n",
      "Epoch 22/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.2245 - acc: 0.2665 - val_loss: 1.5508 - val_acc: 0.3165\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.55021\n",
      "Epoch 23/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0423 - acc: 0.2677 - val_loss: 1.5521 - val_acc: 0.3367\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.55021\n",
      "Epoch 24/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.2892 - acc: 0.2604 - val_loss: 1.5445 - val_acc: 0.3165\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.55021 to 1.54447, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 25/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0783 - acc: 0.2655 - val_loss: 1.5379 - val_acc: 0.3206\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.54447 to 1.53791, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 26/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 6.9092 - acc: 0.2763 - val_loss: 1.5515 - val_acc: 0.3185\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.53791\n",
      "Epoch 27/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0320 - acc: 0.2705 - val_loss: 1.5408 - val_acc: 0.3327\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.53791\n",
      "Epoch 28/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0385 - acc: 0.2702 - val_loss: 1.5292 - val_acc: 0.3488\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.53791 to 1.52923, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 29/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1455 - acc: 0.2700 - val_loss: 1.5308 - val_acc: 0.3286\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.52923\n",
      "Epoch 30/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0572 - acc: 0.2750 - val_loss: 1.5299 - val_acc: 0.3306\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.52923\n",
      "Epoch 31/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.2068 - acc: 0.2662 - val_loss: 1.5244 - val_acc: 0.3367\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.52923 to 1.52443, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 32/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1005 - acc: 0.2838 - val_loss: 1.5276 - val_acc: 0.3347\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.52443\n",
      "Epoch 33/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.2267 - acc: 0.2692 - val_loss: 1.5196 - val_acc: 0.3427\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.52443 to 1.51956, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 34/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.2465 - acc: 0.2750 - val_loss: 1.5350 - val_acc: 0.3306\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.51956\n",
      "Epoch 35/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1632 - acc: 0.2670 - val_loss: 1.5229 - val_acc: 0.3589\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.51956\n",
      "Epoch 36/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1720 - acc: 0.2755 - val_loss: 1.5146 - val_acc: 0.3710\n",
      "\n",
      "Epoch 00036: val_loss improved from 1.51956 to 1.51457, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 37/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0588 - acc: 0.2785 - val_loss: 1.5432 - val_acc: 0.3448\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.51457\n",
      "Epoch 38/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.2469 - acc: 0.2670 - val_loss: 1.5166 - val_acc: 0.3669\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.51457\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.2525 - acc: 0.2738 - val_loss: 1.5279 - val_acc: 0.3286\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.51457\n",
      "Epoch 40/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0880 - acc: 0.2790 - val_loss: 1.5159 - val_acc: 0.3407\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.51457\n",
      "Epoch 41/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1531 - acc: 0.2758 - val_loss: 1.5117 - val_acc: 0.3427\n",
      "\n",
      "Epoch 00041: val_loss improved from 1.51457 to 1.51173, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 42/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0284 - acc: 0.2735 - val_loss: 1.5161 - val_acc: 0.3548\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.51173\n",
      "Epoch 43/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0822 - acc: 0.2803 - val_loss: 1.5382 - val_acc: 0.3226\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.51173\n",
      "Epoch 44/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1688 - acc: 0.2780 - val_loss: 1.5145 - val_acc: 0.3387\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.51173\n",
      "Epoch 45/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1229 - acc: 0.2700 - val_loss: 1.5098 - val_acc: 0.3508\n",
      "\n",
      "Epoch 00045: val_loss improved from 1.51173 to 1.50976, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 46/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1309 - acc: 0.2710 - val_loss: 1.5258 - val_acc: 0.3448\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.50976\n",
      "Epoch 47/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.2813 - acc: 0.2722 - val_loss: 1.5241 - val_acc: 0.3448\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.50976\n",
      "Epoch 48/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1788 - acc: 0.2770 - val_loss: 1.5181 - val_acc: 0.3548\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.50976\n",
      "Epoch 49/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1648 - acc: 0.2720 - val_loss: 1.5064 - val_acc: 0.3488\n",
      "\n",
      "Epoch 00049: val_loss improved from 1.50976 to 1.50641, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 50/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0460 - acc: 0.2896 - val_loss: 1.5071 - val_acc: 0.3488\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.50641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb130162ef0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Modelwith best validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "Try out the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 35.0806%\n"
     ]
    }
   ],
   "source": [
    "actress_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "test_accuracy = 100*np.sum(np.array(actress_predictions)==np.argmax(test_targets, axis=1))/len(actress_predictions)\n",
    "\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
