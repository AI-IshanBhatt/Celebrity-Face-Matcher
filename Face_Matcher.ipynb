{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 :Import Dataset\n",
    "In the code cell below, we import a dataset of actress images. We populate a few variables through the use of the load_files function from the scikit-learn library:\n",
    "\n",
    "1. train_files, valid_files, test_files - numpy arrays containing file paths to images\n",
    "2. train_targets, valid_targets, test_targets - numpy arrays containing onehot-encoded classification labels\n",
    "3. actress_names - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4970 total dog images.\n",
      "\n",
      "There are 3978 training actress images.\n",
      "There are 496 validation actress images.\n",
      "There are 496 test actress images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 5)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('../Celebs/train')\n",
    "valid_files, valid_targets = load_dataset('../Celebs/valid')\n",
    "test_files, test_targets = load_dataset('../Celebs/test')\n",
    "\n",
    "# load list of dog names\n",
    "# dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "#train_files -> paths to the files like dogImages/train/095.Kuvasz/Kuvasz_06442.jpg\n",
    "#train_targets -> 2d array of size*133 categorical all 0s one 1 based on which category the file belongs\n",
    "\n",
    "# print statistics about the dataset\n",
    "# print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training actress images.' % len(train_files))\n",
    "print('There are %d validation actress images.' % len(valid_files))\n",
    "print('There are %d test actress images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process the data\n",
    "Here we are using tensorflow as backend for keras and it requires our images as a certain 4D array a.k.a 4D Tensor with shape.\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where nb_samples corresponds to the total number of images (or samples), and rows, columns, and channels correspond to the number of rows, columns, and channels for each image, respectively.\n",
    "\n",
    "The path_to_tensor takes a string spacifying file location and it does the following operation.\n",
    "\n",
    "1. Resizes the image as (224,224).\n",
    "2. Convert the squared image as an array (3d array)\n",
    "3. Expand the 3d array to 4d array as (1,224,224,3)\n",
    "\n",
    "Another helper function paths_to_tensor takes an array of image file locations as param and in turn calls path_to_tensor on all of them and then vertically stack the output.\n",
    "\n",
    "Here, nb_samples is the number of samples, or number of images, in the supplied array of image paths. It is best to think of nb_samples as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3978/3978 [00:48<00:00, 82.84it/s]\n",
      "100%|██████████| 496/496 [00:05<00:00, 84.52it/s]\n",
      "100%|██████████| 496/496 [00:04<00:00, 100.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "1. Create a CNN from scratch with these tensors\n",
    "2. Create Augmentations and with use of that, And create bottleneck features and save it to S3\n",
    "3. Fetch bottleneck features from S3 and use transfer learning to build a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 2 Create CNN from the scratch\n",
    "Here we will create a CNN from the scratch using Keras. Note that we will not be using any models (vgg,resnet) we will take images as input and train our model.\n",
    "\n",
    "Our target here is to create simplistic model with training,test accuracy above 5%.\n",
    "\n",
    "Be careful with adding too many trainable layers! More parameters means longer training, which means you are more likely to need a GPU to accelerate the training process. Thankfully, Keras provides a handy estimate of the time that each epoch is likely to take; you can extrapolate this estimate to figure out how long it will take for your algorithm to train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 56, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 133)               8645      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 133)               0         \n",
      "=================================================================\n",
      "Total params: 19,189\n",
      "Trainable params: 19,189\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=(224,224,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(train_targets.shape[1], activation='softmax'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3978 samples, validate on 496 samples\n",
      "Epoch 1/50\n",
      "3978/3978 [==============================] - 17s 4ms/step - loss: 7.1750 - acc: 0.2293 - val_loss: 1.6070 - val_acc: 0.3085\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.60702, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 2/50\n",
      "3978/3978 [==============================] - 11s 3ms/step - loss: 7.3807 - acc: 0.2594 - val_loss: 1.5842 - val_acc: 0.3085\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.60702 to 1.58423, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 3/50\n",
      "3978/3978 [==============================] - 11s 3ms/step - loss: 7.1612 - acc: 0.2647 - val_loss: 1.5667 - val_acc: 0.3085\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.58423 to 1.56673, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 4/50\n",
      "3978/3978 [==============================] - 11s 3ms/step - loss: 6.9376 - acc: 0.2715 - val_loss: 1.5664 - val_acc: 0.3085\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.56673 to 1.56638, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 5/50\n",
      "3978/3978 [==============================] - 11s 3ms/step - loss: 7.2873 - acc: 0.2413 - val_loss: 1.5662 - val_acc: 0.3085\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.56638 to 1.56619, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 6/50\n",
      "3978/3978 [==============================] - 11s 3ms/step - loss: 7.0884 - acc: 0.2612 - val_loss: 1.5708 - val_acc: 0.3085\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.56619\n",
      "Epoch 7/50\n",
      "3978/3978 [==============================] - 11s 3ms/step - loss: 6.7606 - acc: 0.2728 - val_loss: 1.5668 - val_acc: 0.3085\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.56619\n",
      "Epoch 8/50\n",
      "3978/3978 [==============================] - 11s 3ms/step - loss: 7.1450 - acc: 0.2695 - val_loss: 1.5643 - val_acc: 0.3065\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.56619 to 1.56426, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 9/50\n",
      "3978/3978 [==============================] - 11s 3ms/step - loss: 7.2799 - acc: 0.2640 - val_loss: 1.5642 - val_acc: 0.3085\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.56426 to 1.56415, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 10/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.3188 - acc: 0.2511 - val_loss: 1.5734 - val_acc: 0.3004\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.56415\n",
      "Epoch 11/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1394 - acc: 0.2552 - val_loss: 1.5743 - val_acc: 0.3085\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.56415\n",
      "Epoch 12/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1616 - acc: 0.2662 - val_loss: 1.5580 - val_acc: 0.3085\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.56415 to 1.55803, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 13/50\n",
      "3978/3978 [==============================] - 11s 3ms/step - loss: 7.1725 - acc: 0.2702 - val_loss: 1.5877 - val_acc: 0.3085\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.55803\n",
      "Epoch 14/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0547 - acc: 0.2577 - val_loss: 1.5693 - val_acc: 0.3165\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.55803\n",
      "Epoch 15/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 6.9962 - acc: 0.2524 - val_loss: 1.5640 - val_acc: 0.3185\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.55803\n",
      "Epoch 16/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.3030 - acc: 0.2574 - val_loss: 1.5502 - val_acc: 0.3105\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.55803 to 1.55021, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 17/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1382 - acc: 0.2559 - val_loss: 1.5600 - val_acc: 0.3145\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.55021\n",
      "Epoch 18/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.2226 - acc: 0.2753 - val_loss: 1.5512 - val_acc: 0.3165\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.55021\n",
      "Epoch 19/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 6.9437 - acc: 0.2705 - val_loss: 1.5657 - val_acc: 0.3206\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.55021\n",
      "Epoch 20/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0826 - acc: 0.2687 - val_loss: 1.5623 - val_acc: 0.2984\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.55021\n",
      "Epoch 21/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0833 - acc: 0.2599 - val_loss: 1.5603 - val_acc: 0.3125\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.55021\n",
      "Epoch 22/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.2245 - acc: 0.2665 - val_loss: 1.5508 - val_acc: 0.3165\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.55021\n",
      "Epoch 23/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0423 - acc: 0.2677 - val_loss: 1.5521 - val_acc: 0.3367\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.55021\n",
      "Epoch 24/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.2892 - acc: 0.2604 - val_loss: 1.5445 - val_acc: 0.3165\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.55021 to 1.54447, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 25/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0783 - acc: 0.2655 - val_loss: 1.5379 - val_acc: 0.3206\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.54447 to 1.53791, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 26/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 6.9092 - acc: 0.2763 - val_loss: 1.5515 - val_acc: 0.3185\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.53791\n",
      "Epoch 27/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0320 - acc: 0.2705 - val_loss: 1.5408 - val_acc: 0.3327\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.53791\n",
      "Epoch 28/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0385 - acc: 0.2702 - val_loss: 1.5292 - val_acc: 0.3488\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.53791 to 1.52923, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 29/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1455 - acc: 0.2700 - val_loss: 1.5308 - val_acc: 0.3286\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.52923\n",
      "Epoch 30/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0572 - acc: 0.2750 - val_loss: 1.5299 - val_acc: 0.3306\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.52923\n",
      "Epoch 31/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.2068 - acc: 0.2662 - val_loss: 1.5244 - val_acc: 0.3367\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.52923 to 1.52443, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 32/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1005 - acc: 0.2838 - val_loss: 1.5276 - val_acc: 0.3347\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.52443\n",
      "Epoch 33/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.2267 - acc: 0.2692 - val_loss: 1.5196 - val_acc: 0.3427\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.52443 to 1.51956, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 34/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.2465 - acc: 0.2750 - val_loss: 1.5350 - val_acc: 0.3306\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.51956\n",
      "Epoch 35/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1632 - acc: 0.2670 - val_loss: 1.5229 - val_acc: 0.3589\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.51956\n",
      "Epoch 36/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1720 - acc: 0.2755 - val_loss: 1.5146 - val_acc: 0.3710\n",
      "\n",
      "Epoch 00036: val_loss improved from 1.51956 to 1.51457, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 37/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0588 - acc: 0.2785 - val_loss: 1.5432 - val_acc: 0.3448\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.51457\n",
      "Epoch 38/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.2469 - acc: 0.2670 - val_loss: 1.5166 - val_acc: 0.3669\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.51457\n",
      "Epoch 39/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.2525 - acc: 0.2738 - val_loss: 1.5279 - val_acc: 0.3286\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.51457\n",
      "Epoch 40/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0880 - acc: 0.2790 - val_loss: 1.5159 - val_acc: 0.3407\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.51457\n",
      "Epoch 41/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1531 - acc: 0.2758 - val_loss: 1.5117 - val_acc: 0.3427\n",
      "\n",
      "Epoch 00041: val_loss improved from 1.51457 to 1.51173, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 42/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0284 - acc: 0.2735 - val_loss: 1.5161 - val_acc: 0.3548\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.51173\n",
      "Epoch 43/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0822 - acc: 0.2803 - val_loss: 1.5382 - val_acc: 0.3226\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.51173\n",
      "Epoch 44/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1688 - acc: 0.2780 - val_loss: 1.5145 - val_acc: 0.3387\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.51173\n",
      "Epoch 45/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1229 - acc: 0.2700 - val_loss: 1.5098 - val_acc: 0.3508\n",
      "\n",
      "Epoch 00045: val_loss improved from 1.51173 to 1.50976, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 46/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1309 - acc: 0.2710 - val_loss: 1.5258 - val_acc: 0.3448\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.50976\n",
      "Epoch 47/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.2813 - acc: 0.2722 - val_loss: 1.5241 - val_acc: 0.3448\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.50976\n",
      "Epoch 48/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1788 - acc: 0.2770 - val_loss: 1.5181 - val_acc: 0.3548\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.50976\n",
      "Epoch 49/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.1648 - acc: 0.2720 - val_loss: 1.5064 - val_acc: 0.3488\n",
      "\n",
      "Epoch 00049: val_loss improved from 1.50976 to 1.50641, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 50/50\n",
      "3978/3978 [==============================] - 12s 3ms/step - loss: 7.0460 - acc: 0.2896 - val_loss: 1.5071 - val_acc: 0.3488\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.50641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb130162ef0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Modelwith best validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "Try out the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 35.0806%\n"
     ]
    }
   ],
   "source": [
    "actress_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "test_accuracy = 100*np.sum(np.array(actress_predictions)==np.argmax(test_targets, axis=1))/len(actress_predictions)\n",
    "\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bottleneck features from Augmented Data\n",
    "As we have very less number of samples, We will augment the data and compute bottleneck features for xception model.\n",
    "\n",
    "We will save it and then later use it for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3978 images belonging to 5 classes.\n",
      "Found 496 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras.applications.xception as xception\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "#load the model\n",
    "# model = xception.Xception(weights='imagenet', include_top=False)\n",
    "xception_image_size = (224,224)\n",
    "batch_size = 20\n",
    "\n",
    "# This is the augmentation configuration.\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.05,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "                '../Celebs/train',\n",
    "                target_size=xception_image_size,\n",
    "                batch_size=batch_size)\n",
    "\n",
    "valid_generator = datagen.flow_from_directory(\n",
    "                '../Celebs/valid',\n",
    "                target_size=xception_image_size,\n",
    "                batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeaturesForAugmentedData(generator, n_iterations):\n",
    "    \n",
    "    list_of_features=[]\n",
    "    list_of_targets=[]\n",
    "    for i in trange(n_iterations):\n",
    "        x,y = generator.next()\n",
    "        # preprocess and compute bottleneck features\n",
    "        f = model.predict(xception.preprocess_input(x))\n",
    "        # append features and labels to lists\n",
    "        list_of_features.append(f)\n",
    "        list_of_targets.append(y)\n",
    "            \n",
    "    # return features and targets as numpy arrays\n",
    "    return np.vstack(list_of_features),np.vstack(list_of_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:18<00:00,  1.03s/it]\n",
      "100%|██████████| 25/25 [00:24<00:00,  1.00it/s]\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = getFeaturesForAugmentedData(train_generator, 5000//batch_size)\n",
    "valid_x, valid_y = getFeaturesForAugmentedData(valid_generator, 500//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading test files as it is, Not using generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 496/496 [00:21<00:00, 23.07it/s]\n"
     ]
    }
   ],
   "source": [
    "def path_to_Xception_feature(img_path):\n",
    "    img = image.load_img(img_path, target_size=xception_image_size)\n",
    "    tensor = xception.preprocess_input(np.expand_dims(image.img_to_array(img), axis=0))\n",
    "    return model.predict(tensor)\n",
    "\n",
    "def paths_to_Xception_features(img_paths):\n",
    "    list_of_features = [path_to_Xception_feature(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_features)\n",
    "\n",
    "test_x = paths_to_Xception_features(test_files)\n",
    "test_y = test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train and test features for further use\n",
    "np.savez('ActressAugmentedData.npz',train_x=train_x, train_y=train_y,\\\n",
    "         valid_x=valid_x, valid_y=valid_y, test_x=test_x, test_y=test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7309dbc1a4f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ActressAugmentedData.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_Xception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvalid_Xception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid_x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid_y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_Xception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    233\u001b[0m                 return format.read_array(bytes,\n\u001b[1;32m    234\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                                          pickle_kwargs=self.pickle_kwargs)\n\u001b[0m\u001b[1;32m    236\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m                     \u001b[0mread_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_read_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m                     \u001b[0mread_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_count\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m                     array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[1;32m    685\u001b[0m                                                              count=read_count)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;31m# done about that.  note that regular files can't be non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    917\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mZIP_STORED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m_read2\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    704\u001b[0m                         \"Close the writing handle before trying to read.\")\n\u001b[1;32m    705\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Load the data \n",
    "fh = np.load('ActressAugmentedData.npz')\n",
    "train_Xception, train_targets = fh['train_x'], fh['train_y']\n",
    "valid_Xception, valid_targets = fh['valid_x'], fh['valid_y']\n",
    "test_Xception, test_targets = fh['test_x'], fh['test_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model on top of this and summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_3 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 1,051,653\n",
      "Trainable params: 1,051,653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "Xception_model = Sequential()\n",
    "Xception_model.add(GlobalAveragePooling2D(input_shape=train_x.shape[1:]))\n",
    "Xception_model.add(Dropout(0.4))\n",
    "Xception_model.add(Dense(512, activation='relu'))\n",
    "Xception_model.add(Dropout(0.4)) #Dropout between input and hidden\n",
    "Xception_model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "\n",
    "Xception_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xception_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "Train your model in the code cell below. Use model checkpointing to save the model that attains the best validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4998 samples, validate on 496 samples\n",
      "Epoch 1/50\n",
      " - 4s - loss: 1.5011 - acc: 0.3603 - val_loss: 1.4188 - val_acc: 0.3871\n",
      "Epoch 2/50\n",
      " - 3s - loss: 1.3692 - acc: 0.4242 - val_loss: 1.4318 - val_acc: 0.3710\n",
      "Epoch 3/50\n",
      " - 3s - loss: 1.3163 - acc: 0.4600 - val_loss: 1.3790 - val_acc: 0.4274\n",
      "Epoch 4/50\n",
      " - 3s - loss: 1.2500 - acc: 0.4854 - val_loss: 1.3696 - val_acc: 0.4496\n",
      "Epoch 5/50\n",
      " - 3s - loss: 1.2115 - acc: 0.5104 - val_loss: 1.3671 - val_acc: 0.4073\n",
      "Epoch 6/50\n",
      " - 3s - loss: 1.1627 - acc: 0.5350 - val_loss: 1.3762 - val_acc: 0.4234\n",
      "Epoch 7/50\n",
      " - 3s - loss: 1.1171 - acc: 0.5606 - val_loss: 1.4107 - val_acc: 0.4073\n",
      "Epoch 8/50\n",
      " - 3s - loss: 1.0987 - acc: 0.5590 - val_loss: 1.3818 - val_acc: 0.4496\n",
      "Epoch 9/50\n",
      " - 3s - loss: 1.0362 - acc: 0.5808 - val_loss: 1.3533 - val_acc: 0.4254\n",
      "Epoch 10/50\n",
      " - 3s - loss: 1.0148 - acc: 0.6004 - val_loss: 1.3209 - val_acc: 0.4395\n",
      "Epoch 11/50\n",
      " - 3s - loss: 0.9612 - acc: 0.6269 - val_loss: 1.3395 - val_acc: 0.4819\n",
      "Epoch 12/50\n",
      " - 3s - loss: 0.8963 - acc: 0.6575 - val_loss: 1.3137 - val_acc: 0.4536\n",
      "Epoch 13/50\n",
      " - 3s - loss: 0.8679 - acc: 0.6697 - val_loss: 1.3874 - val_acc: 0.4536\n",
      "Epoch 14/50\n",
      " - 3s - loss: 0.8531 - acc: 0.6675 - val_loss: 1.3248 - val_acc: 0.4758\n",
      "Epoch 15/50\n",
      " - 3s - loss: 0.8038 - acc: 0.6951 - val_loss: 1.3019 - val_acc: 0.4798\n",
      "Epoch 16/50\n",
      " - 3s - loss: 0.7830 - acc: 0.7009 - val_loss: 1.3379 - val_acc: 0.4657\n",
      "Epoch 17/50\n",
      " - 3s - loss: 0.7475 - acc: 0.7151 - val_loss: 1.3562 - val_acc: 0.4435\n",
      "Epoch 18/50\n",
      " - 3s - loss: 0.6892 - acc: 0.7357 - val_loss: 1.3620 - val_acc: 0.4637\n",
      "Epoch 19/50\n",
      " - 3s - loss: 0.6685 - acc: 0.7507 - val_loss: 1.3586 - val_acc: 0.4698\n",
      "Epoch 20/50\n",
      " - 3s - loss: 0.6281 - acc: 0.7671 - val_loss: 1.3867 - val_acc: 0.4778\n",
      "Epoch 21/50\n",
      " - 3s - loss: 0.6440 - acc: 0.7649 - val_loss: 1.4377 - val_acc: 0.4919\n",
      "Epoch 22/50\n",
      " - 3s - loss: 0.6107 - acc: 0.7751 - val_loss: 1.3444 - val_acc: 0.4798\n",
      "Epoch 23/50\n",
      " - 3s - loss: 0.5841 - acc: 0.7813 - val_loss: 1.4178 - val_acc: 0.4940\n",
      "Epoch 24/50\n",
      " - 3s - loss: 0.5696 - acc: 0.7889 - val_loss: 1.3976 - val_acc: 0.4778\n",
      "Epoch 25/50\n",
      " - 3s - loss: 0.5525 - acc: 0.8003 - val_loss: 1.4109 - val_acc: 0.4859\n",
      "Epoch 26/50\n",
      " - 3s - loss: 0.4991 - acc: 0.8153 - val_loss: 1.3698 - val_acc: 0.5121\n",
      "Epoch 27/50\n",
      " - 3s - loss: 0.5027 - acc: 0.8173 - val_loss: 1.4010 - val_acc: 0.4879\n",
      "Epoch 28/50\n",
      " - 3s - loss: 0.5114 - acc: 0.8139 - val_loss: 1.3847 - val_acc: 0.4899\n",
      "Epoch 29/50\n",
      " - 3s - loss: 0.4593 - acc: 0.8391 - val_loss: 1.4443 - val_acc: 0.4960\n",
      "Epoch 30/50\n",
      " - 3s - loss: 0.4661 - acc: 0.8319 - val_loss: 1.3990 - val_acc: 0.5081\n",
      "Epoch 31/50\n",
      " - 3s - loss: 0.4713 - acc: 0.8265 - val_loss: 1.4460 - val_acc: 0.4919\n",
      "Epoch 32/50\n",
      " - 3s - loss: 0.4528 - acc: 0.8395 - val_loss: 1.4544 - val_acc: 0.5202\n",
      "Epoch 33/50\n",
      " - 3s - loss: 0.4338 - acc: 0.8423 - val_loss: 1.4424 - val_acc: 0.5121\n",
      "Epoch 34/50\n",
      " - 3s - loss: 0.4353 - acc: 0.8415 - val_loss: 1.4902 - val_acc: 0.5081\n",
      "Epoch 35/50\n",
      " - 3s - loss: 0.4195 - acc: 0.8467 - val_loss: 1.4265 - val_acc: 0.5040\n",
      "Epoch 36/50\n",
      " - 3s - loss: 0.3911 - acc: 0.8623 - val_loss: 1.4857 - val_acc: 0.5141\n",
      "Epoch 37/50\n",
      " - 3s - loss: 0.3900 - acc: 0.8655 - val_loss: 1.4047 - val_acc: 0.5000\n",
      "Epoch 38/50\n",
      " - 3s - loss: 0.3917 - acc: 0.8615 - val_loss: 1.4671 - val_acc: 0.4919\n",
      "Epoch 39/50\n",
      " - 3s - loss: 0.3764 - acc: 0.8689 - val_loss: 1.4634 - val_acc: 0.4899\n",
      "Epoch 40/50\n",
      " - 3s - loss: 0.3618 - acc: 0.8681 - val_loss: 1.4481 - val_acc: 0.5141\n",
      "Epoch 41/50\n",
      " - 3s - loss: 0.3574 - acc: 0.8733 - val_loss: 1.5278 - val_acc: 0.4899\n",
      "Epoch 42/50\n",
      " - 3s - loss: 0.3612 - acc: 0.8727 - val_loss: 1.5195 - val_acc: 0.5040\n",
      "Epoch 43/50\n",
      " - 3s - loss: 0.3558 - acc: 0.8741 - val_loss: 1.4736 - val_acc: 0.5121\n",
      "Epoch 44/50\n",
      " - 3s - loss: 0.3446 - acc: 0.8810 - val_loss: 1.5615 - val_acc: 0.5101\n",
      "Epoch 45/50\n",
      " - 3s - loss: 0.3224 - acc: 0.8838 - val_loss: 1.5007 - val_acc: 0.4798\n",
      "Epoch 46/50\n",
      " - 3s - loss: 0.3098 - acc: 0.8900 - val_loss: 1.6149 - val_acc: 0.4657\n",
      "Epoch 47/50\n",
      " - 3s - loss: 0.3252 - acc: 0.8828 - val_loss: 1.5612 - val_acc: 0.4919\n",
      "Epoch 48/50\n",
      " - 3s - loss: 0.3030 - acc: 0.8912 - val_loss: 1.5572 - val_acc: 0.4960\n",
      "Epoch 49/50\n",
      " - 3s - loss: 0.3183 - acc: 0.8868 - val_loss: 1.5633 - val_acc: 0.5000\n",
      "Epoch 50/50\n",
      " - 3s - loss: 0.3084 - acc: 0.8870 - val_loss: 1.5695 - val_acc: 0.4859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4fb7c86668>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.Xception.hdf5', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "Xception_model.fit(train_x, train_y,\n",
    "                   validation_data=(valid_x, valid_y),\n",
    "                   epochs=50, batch_size=batch_size, callbacks=[checkpointer], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xception model test accuracy: 50.8065%\n"
     ]
    }
   ],
   "source": [
    "Xception_model.load_weights('saved_models/weights.best.Xception.hdf5')\n",
    "Xception_predictions = [np.argmax(Xception_model.predict(\n",
    "    np.expand_dims(feature, axis=0))) for feature in test_x]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(Xception_predictions)==\n",
    "                           np.argmax(test_y, axis=1))/len(Xception_predictions)\n",
    "print('Xception model test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's fine tune the model\n",
    "From above we see that because our dataset is small and it has less overlapping with Imagenet dataset, We suffer from overfitting. \n",
    "\n",
    "Our training accuracy shoots up but validation and test accuracy is not going up. \n",
    "\n",
    "So, now we will load resnet model , chop off last convolutional layers , add our conv layers and then train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x7f801e38bd30> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f801e38b7f0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f801e38bc88> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f7ffa379550> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7ffa2ef1d0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7ffa304b38> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f801fb0b2e8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f801fb386a0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f801fb38630> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f801fae67f0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f801fa932b0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f801fa436a0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f801fa59d30> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f801fa71fd0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f801fa08630> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f801fa369b0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f801f9e2470> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f801f991c50> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f801f991f28> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f801f9bf0f0> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f801f955400> True\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f801f905208> True\n"
     ]
    }
   ],
   "source": [
    "import keras.applications.vgg19 as vgg19 #preprocess_input\n",
    "image_size = 224\n",
    "VGG19_model = vgg19.VGG19(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
    "\n",
    "for layer in VGG19_model.layers[:-3]:\n",
    "    layer.trainable=False\n",
    "\n",
    "for layer in VGG19_model.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Model)                (None, 7, 7, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_5 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 20,554,821\n",
      "Trainable params: 5,250,053\n",
      "Non-trainable params: 15,304,768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "new_model = models.Sequential()\n",
    "new_model.add(VGG19_model)\n",
    "\n",
    "# BEFORE ADDING NEW LAYERS WE WILL COMPUTE BOTTLENECK FEATURES FOR THIS TWISTED (WITHOUT LAST 2 LAYER) NETWORK.\n",
    "# And add the stuff ,new layers and all and train it.\n",
    "\n",
    "new_model.add(layers.GlobalAveragePooling2D())\n",
    "new_model.add(layers.Dense(1024, activation='relu'))\n",
    "new_model.add(layers.Dropout(0.5))\n",
    "new_model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "# Show a summary of the model. Check the number of trainable parameters\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3978 images belonging to 5 classes.\n",
      "Found 496 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=20,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "# Change the batchsize according to your system RAM\n",
    "train_batchsize = 100\n",
    "val_batchsize = 10\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '../Celebs/train',\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=train_batchsize,\n",
    "        class_mode='categorical')\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        '../Celebs/valid',\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "40/39 [==============================] - 104s 3s/step - loss: 1.6594 - acc: 0.2826 - val_loss: 1.5632 - val_acc: 0.3468\n",
      "Epoch 2/25\n",
      "40/39 [==============================] - 98s 2s/step - loss: 1.5384 - acc: 0.3514 - val_loss: 1.5019 - val_acc: 0.3690\n",
      "Epoch 3/25\n",
      "40/39 [==============================] - 98s 2s/step - loss: 1.4989 - acc: 0.3720 - val_loss: 1.4680 - val_acc: 0.3871\n",
      "Epoch 4/25\n",
      "40/39 [==============================] - 98s 2s/step - loss: 1.4605 - acc: 0.3970 - val_loss: 1.4525 - val_acc: 0.3891\n",
      "Epoch 5/25\n",
      "40/39 [==============================] - 97s 2s/step - loss: 1.4481 - acc: 0.3944 - val_loss: 1.4347 - val_acc: 0.3972\n",
      "Epoch 6/25\n",
      "40/39 [==============================] - 98s 2s/step - loss: 1.4130 - acc: 0.4046 - val_loss: 1.4180 - val_acc: 0.4032\n",
      "Epoch 7/25\n",
      "40/39 [==============================] - 98s 2s/step - loss: 1.3972 - acc: 0.4115 - val_loss: 1.3594 - val_acc: 0.4173\n",
      "Epoch 8/25\n",
      "40/39 [==============================] - 96s 2s/step - loss: 1.3405 - acc: 0.4370 - val_loss: 1.3295 - val_acc: 0.4476\n",
      "Epoch 9/25\n",
      "40/39 [==============================] - 97s 2s/step - loss: 1.2987 - acc: 0.4649 - val_loss: 1.3205 - val_acc: 0.4677\n",
      "Epoch 10/25\n",
      "40/39 [==============================] - 98s 2s/step - loss: 1.2500 - acc: 0.4888 - val_loss: 1.2102 - val_acc: 0.4960\n",
      "Epoch 11/25\n",
      "40/39 [==============================] - 97s 2s/step - loss: 1.1880 - acc: 0.5207 - val_loss: 1.1680 - val_acc: 0.5685\n",
      "Epoch 12/25\n",
      "40/39 [==============================] - 98s 2s/step - loss: 1.1288 - acc: 0.5538 - val_loss: 1.0880 - val_acc: 0.5827\n",
      "Epoch 13/25\n",
      "40/39 [==============================] - 98s 2s/step - loss: 1.0384 - acc: 0.6005 - val_loss: 0.9939 - val_acc: 0.6371\n",
      "Epoch 14/25\n",
      "40/39 [==============================] - 98s 2s/step - loss: 0.9914 - acc: 0.6213 - val_loss: 1.0161 - val_acc: 0.6169\n",
      "Epoch 15/25\n",
      "40/39 [==============================] - 97s 2s/step - loss: 0.9172 - acc: 0.6477 - val_loss: 0.9468 - val_acc: 0.6573\n",
      "Epoch 16/25\n",
      "40/39 [==============================] - 98s 2s/step - loss: 0.8698 - acc: 0.6809 - val_loss: 0.9570 - val_acc: 0.6573\n",
      "Epoch 17/25\n",
      "40/39 [==============================] - 98s 2s/step - loss: 0.8331 - acc: 0.6917 - val_loss: 0.9569 - val_acc: 0.6391\n",
      "Epoch 18/25\n",
      "40/39 [==============================] - 98s 2s/step - loss: 0.7611 - acc: 0.7180 - val_loss: 0.9044 - val_acc: 0.6774\n",
      "Epoch 19/25\n",
      "40/39 [==============================] - 96s 2s/step - loss: 0.7135 - acc: 0.7377 - val_loss: 0.9416 - val_acc: 0.6794\n",
      "Epoch 20/25\n",
      "40/39 [==============================] - 98s 2s/step - loss: 0.6806 - acc: 0.7488 - val_loss: 0.9436 - val_acc: 0.6774\n",
      "Epoch 21/25\n",
      "40/39 [==============================] - 97s 2s/step - loss: 0.6303 - acc: 0.7748 - val_loss: 0.9980 - val_acc: 0.6875\n",
      "Epoch 22/25\n",
      "40/39 [==============================] - 99s 2s/step - loss: 0.5999 - acc: 0.7805 - val_loss: 0.9457 - val_acc: 0.6815\n",
      "Epoch 23/25\n",
      "40/39 [==============================] - 97s 2s/step - loss: 0.5862 - acc: 0.7874 - val_loss: 0.9624 - val_acc: 0.7016\n",
      "Epoch 24/25\n",
      "40/39 [==============================] - 98s 2s/step - loss: 0.5049 - acc: 0.8189 - val_loss: 0.9891 - val_acc: 0.7117\n",
      "Epoch 25/25\n",
      "40/39 [==============================] - 97s 2s/step - loss: 0.4930 - acc: 0.8259 - val_loss: 1.0718 - val_acc: 0.6875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f801f5c62e8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.vgg19_2.hdf5', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "new_model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=train_generator.samples/train_generator.batch_size ,\n",
    "      epochs=25,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
    "      callbacks=[checkpointer],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(496, 224, 224, 3)\n",
      "(496, 5)\n"
     ]
    }
   ],
   "source": [
    "def path_to_VGG19_feature(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224,224))\n",
    "    tensor = np.expand_dims(image.img_to_array(img), axis=0)\n",
    "    return tensor\n",
    "\n",
    "def paths_to_VGG19_features(image_paths):\n",
    "    list_of_features = [path_to_VGG19_feature(image_path) for image_path in image_paths]\n",
    "    return np.vstack(list_of_features)\n",
    "\n",
    "test_x = paths_to_VGG19_features(test_files).astype('float32')/255\n",
    "test_y = test_targets\n",
    "\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHTS LOADED\n",
      "Test accuracy: 66.1290%\n"
     ]
    }
   ],
   "source": [
    "new_model.load_weights('saved_models/weights.best.vgg19_2.hdf5')\n",
    "print(\"WEIGHTS LOADED\")\n",
    "VGG19_predictions = [np.argmax(new_model.predict(np.expand_dims(feature, axis=0))) for feature in test_x]\n",
    "#Keras expects the input tensor to be in (1,224,224,3). \n",
    "#VGG19_predictions will be an array with indexes , we will compare both indexes in next line\n",
    "\n",
    "test_accuracy = 100*np.sum(np.array(VGG19_predictions) == np.argmax(test_y, axis=1))/len(VGG19_predictions)\n",
    "#test_y will be like (0,0,1,0,0) take out the max, Or you can compare its element to 1 and get index as well.\n",
    "\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
